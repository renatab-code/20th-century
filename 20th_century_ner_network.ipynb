{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ac56514-0be1-4632-8d72-11171565fbb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/20th_century/lib/python3.10/site-packages/spacy/cli/info.py:3: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  import pkg_resources\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spaCy OK: 3.4.3\n",
      "NetworkX OK: 2.8.8\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "# load spaCy model (NER-capable)\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "print(\"spaCy OK:\", spacy.__version__)\n",
    "print(\"NetworkX OK:\", nx.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "22e5450e-5bd6-4382-87fa-ce9890173143",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CWD: /Users/renatabatista/Other Docs/Germany/CareerFoundry/Data Specialization/JupyterLab/20th-century\n",
      "Characters in text: 96901\n",
      "This is a timeline of the 20th century .\n",
      "1900s\n",
      "1901\n",
      "January 1 : The Australian colonies federate .\n",
      "January 22 : Edward VII became King of England and India after Queen Victoria 's death.\n",
      "March 2 : The Platt Amendment provides for Cuban independence in exchange for the withdrawal of American troops.\n",
      "June : Emily Hobhouse reports on the poor conditions in 45 British internment camps for Boer women and children in South Africa .\n",
      "September 6 : The assassination of William McKinley ushered in office Vice President Theodore Roosevelt after McKinley's death on September 14.\n",
      "September 7 : The Eight-Na\n"
     ]
    }
   ],
   "source": [
    "# Path sanity check (optional)\n",
    "import os\n",
    "print(\"CWD:\", os.getcwd())\n",
    "\n",
    "# Load the text\n",
    "text_path = Path(\"20th_century_events.txt\")\n",
    "assert text_path.exists(), \"Couldn't find 20th_century_events.txt in this folder.\"\n",
    "text = text_path.read_text(encoding=\"utf-8\")\n",
    "\n",
    "print(\"Characters in text:\", len(text))\n",
    "print(text[:600])  # quick preview\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c88fd3dd-60d2-441f-bbb0-ef75b9dfb3b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique non-ASCII chars: ['½', 'É', 'Ö', 'Ø', 'á', 'â', 'é', 'í', 'ó', 'ö', 'ú', 'ü', 'ć', 'ę', 'ł', 'ń', 'ō', 'š', 'ș', '–']\n",
      "Count non-ASCII: 243\n",
      "Odd char counts: [(\"'–'\", 193)]\n",
      "Reference markers found: 0\n",
      "\n",
      "Sample lines:\n",
      " • This is a timeline of the 20th century .\n",
      " • 1900s\n",
      " • 1901\n",
      " • January 1 : The Australian colonies federate .\n",
      " • January 22 : Edward VII became King of England and India after Queen Victoria 's death.\n",
      " • March 2 : The Platt Amendment provides for Cuban independence in exchange for the withdrawal of American troops.\n",
      " • June : Emily Hobhouse reports on the poor conditions in 45 British internment camps for Boer women and children in South Africa .\n",
      " • September 6 : The assassination of William McKinley ushered in office Vice President Theodore Roosevelt after McKinley's death on September 14.\n"
     ]
    }
   ],
   "source": [
    "import re, unicodedata\n",
    "from collections import Counter\n",
    "from pathlib import Path\n",
    "\n",
    "src = Path(\"20th_century_events.txt\")\n",
    "text = src.read_text(encoding=\"utf-8\")\n",
    "\n",
    "# Non-ASCII characters present?\n",
    "non_ascii = sorted({ch for ch in text if ord(ch) > 127})\n",
    "print(\"Unique non-ASCII chars:\", non_ascii[:50])\n",
    "print(\"Count non-ASCII:\", sum(ord(ch)>127 for ch in text))\n",
    "\n",
    "# Show the 15 most common weird whitespace/hyphen characters, if any\n",
    "odd_chars = ['\\u00A0','\\u2009','\\u2010','\\u2011','\\u2013','\\u2014','\\u2018','\\u2019','\\u201C','\\u201D','\\u2022','\\u2212']\n",
    "present = [(repr(c), text.count(c)) for c in odd_chars if c in text]\n",
    "print(\"Odd char counts:\", present)\n",
    "\n",
    "# Strip refs like [1], [2]...\n",
    "refs = re.findall(r\"\\[\\d+\\]\", text)\n",
    "print(\"Reference markers found:\", len(refs))\n",
    "\n",
    "# Sample of sentences that include years (quick sanity peek)\n",
    "print(\"\\nSample lines:\")\n",
    "for line in text.splitlines()[:8]:\n",
    "    print(\" •\", line)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4a82dde3-e49f-445b-b386-c18fca93f65f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before chars: 96901 After chars: 95108\n",
      "Preview:\n",
      " This is a timeline of the 20th century.\n",
      "1900s\n",
      "1901\n",
      "January 1: The Australian colonies federate.\n",
      "January 22: Edward VII became King of England and India after Queen Victoria 's death.\n",
      "March 2: The Platt Amendment provides for Cuban independence in exchange for the withdrawal of American troops.\n",
      "June: Emily Hobhouse reports on the poor conditions in 45 British internment camps for Boer women and children in South Africa.\n",
      "September 6: The assassination of William McKinley ushered in office Vice President Theodore Roosevelt after McKinley's death on September 14.\n",
      "September 7: The Eight-Nation Alli\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def clean_text(s: str) -> str:\n",
    "    # Normalize unicode to NFC\n",
    "    s = unicodedata.normalize(\"NFC\", s)\n",
    "\n",
    "    # Replace common typography with ASCII equivalents\n",
    "    replacements = {\n",
    "        \"\\u2018\":\"'\", \"\\u2019\":\"'\", \"\\u201C\":'\"', \"\\u201D\":'\"',\n",
    "        \"\\u2013\":\"-\",  \"\\u2014\":\"-\",  \"\\u2212\":\"-\",\n",
    "        \"\\u00A0\":\" \",  \"\\u2009\":\" \",  \"\\u2010\":\"-\", \"\\u2011\":\"-\",\n",
    "        \"\\u2022\":\"•\",  # keep bullets but normalize\n",
    "    }\n",
    "    for k,v in replacements.items():\n",
    "        s = s.replace(k,v)\n",
    "\n",
    "    # Remove citation markers like [12], [a], [clarification needed]\n",
    "    s = re.sub(r\"\\[[^\\]]*?\\]\", \"\", s)\n",
    "\n",
    "    # Collapse multiple spaces/newlines\n",
    "    s = re.sub(r\"[ \\t]+\", \" \", s)\n",
    "    s = re.sub(r\"\\n{3,}\", \"\\n\\n\", s)\n",
    "\n",
    "    # Trim spaces before punctuation\n",
    "    s = re.sub(r\"\\s+([,.;:!?])\", r\"\\1\", s)\n",
    "\n",
    "    # Ensure year headings like \"1901\" stand alone (optional formatting)\n",
    "    s = re.sub(r\"\\n(\\d{4})(?!\\d)\", r\"\\n\\1\", s)\n",
    "\n",
    "    return s.strip()\n",
    "\n",
    "clean = clean_text(text)\n",
    "\n",
    "print(\"Before chars:\", len(text), \"After chars:\", len(clean))\n",
    "print(\"Preview:\\n\", clean[:600])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef5e279-209f-40b4-ac0d-9423366a26b4",
   "metadata": {},
   "source": [
    "After run a quick diagnosis and check for odd characters,I cleaned the text and preserved the mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9b57aa34-710d-46be-944a-88cee8a9f9e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Countries found: 44\n",
      "['argentina', 'australia', 'austria', 'belgium', 'brazil', 'canada', 'chile', 'china', 'czechoslovakia', 'denmark', 'egypt', 'finland', 'france', 'germany', 'greece', 'hungary', 'india', 'indonesia', 'iran', 'iraq'] ...\n",
      "Countries not found (check aliases/casing): ['yugoslavia']\n"
     ]
    }
   ],
   "source": [
    "# Minimal example: extend with your existing alias dict from earlier\n",
    "countries = [\n",
    "    \"united states\",\"united kingdom\",\"germany\",\"france\",\"italy\",\"spain\",\n",
    "    \"russia\",\"soviet union\",\"china\",\"japan\",\"india\",\"canada\",\"australia\",\n",
    "    \"poland\",\"austria\",\"hungary\",\"netherlands\",\"belgium\",\"switzerland\",\n",
    "    \"sweden\",\"norway\",\"denmark\",\"finland\",\"greece\",\"portugal\",\"ireland\",\n",
    "    \"czechoslovakia\",\"yugoslavia\",\"turkey\",\"egypt\",\"iran\",\"iraq\",\"israel\",\n",
    "    \"mexico\",\"brazil\",\"argentina\",\"chile\",\"south africa\",\"pakistan\",\n",
    "    \"south korea\",\"north korea\",\"vietnam\",\"indonesia\",\"philippines\",\"thailand\"\n",
    "]\n",
    "\n",
    "aliases = {\n",
    "    \"usa\":\"united states\",\"u.s.\":\"united states\",\"u.s.a.\":\"united states\",\"america\":\"united states\",\n",
    "    \"uk\":\"united kingdom\",\"u.k.\":\"united kingdom\",\"britain\":\"united kingdom\",\"great britain\":\"united kingdom\",\"england\":\"united kingdom\",\n",
    "    \"ussr\":\"soviet union\",\"u.s.s.r.\":\"soviet union\",\"union of soviet socialist republics\":\"soviet union\",\n",
    "    \"west germany\":\"germany\",\"east germany\":\"germany\",\"frg\":\"germany\",\"gdr\":\"germany\",\n",
    "    \"prc\":\"china\",\"people's republic of china\":\"china\",\"roc\":\"china\",\"republic of china\":\"china\",\n",
    "    \"ottoman empire\":\"turkey\",\"ottoman\":\"turkey\",\n",
    "    \"siam\":\"thailand\",\"burma\":\"myanmar\",\"ceylon\":\"sri lanka\",\"holland\":\"netherlands\",\n",
    "}\n",
    "\n",
    "# normalize once\n",
    "clean_lc = clean.lower()\n",
    "\n",
    "# replace aliases first (longest first avoids partial overlaps)\n",
    "for alias, canon in sorted(aliases.items(), key=lambda kv: len(kv[0]), reverse=True):\n",
    "    clean_lc = re.sub(rf\"\\b{re.escape(alias)}\\b\", canon, clean_lc, flags=re.IGNORECASE)\n",
    "\n",
    "# show which canonicals are present at least once\n",
    "present = [c for c in countries if re.search(rf\"\\b{re.escape(c)}\\b\", clean_lc)]\n",
    "missing  = [c for c in countries if c not in present]\n",
    "\n",
    "print(\"Countries found:\", len(present))\n",
    "print(sorted(present)[:20], \"...\")\n",
    "print(\"Countries not found (check aliases/casing):\", missing[:20])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ef0e4d37-b0e1-47f2-8cb0-361aaa687970",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: /Users/renatabatista/Other Docs/Germany/CareerFoundry/Data Specialization/JupyterLab/20th-century/20th_century_events_clean.txt | size: 95158 bytes\n"
     ]
    }
   ],
   "source": [
    "out = Path(\"20th_century_events_clean.txt\")\n",
    "out.write_text(clean, encoding=\"utf-8\")\n",
    "print(\"Saved:\", out.resolve(), \"| size:\", out.stat().st_size, \"bytes\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d2ed14a-6914-4b85-b501-13d611be7f27",
   "metadata": {},
   "source": [
    "Data wrangling observations\n",
    "\n",
    "- The raw Wikipedia text contained typographic quotes (“ ” ‘ ’), en/em dashes (– —), and non-breaking spaces. It also included citation markers like [12].\n",
    "- I normalized the text to ASCII-friendly forms (quotes → ' / \", dashes → -), removed citation markers, and collapsed excess whitespace.\n",
    "- Country names in the article appear under multiple variants (e.g., “UK”, “Britain”, “Great Britain”, “U.S.”, “America”, “USSR”). I normalized the text by replacing aliases with canonical names (e.g., uk → united kingdom, ussr → soviet union).\n",
    "- After cleaning + alias replacement, I verified which countries from my lookup were present and added a few extra aliases where needed.\n",
    "\n",
    "Outcome: \n",
    "the cleaned file 20th_century_events_clean.txt is now consistent, free of distracting typography/refs, and aligned with the canonical country names I’ll use for NER and the network graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b8c85e93-d579-4ac2-abcc-540493439257",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(95108,\n",
       " \"This is a timeline of the 20th century.\\n1900s\\n1901\\nJanuary 1: The Australian colonies federate.\\nJanuary 22: Edward VII became King of England and India after Queen Victoria 's death.\\nMarch 2: The Platt Amendment provides for Cuban independence in exchange for the withdrawal of American troops.\\nJune:\")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "from pathlib import Path\n",
    "\n",
    "# Use the small English model (already installed earlier)\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Read your cleaned file (use the raw file if you prefer)\n",
    "text_path = Path(\"20th_century_events_clean.txt\")  # or \"20th_century_events.txt\"\n",
    "assert text_path.exists(), f\"Missing file: {text_path.resolve()}\"\n",
    "text = text_path.read_text(encoding=\"utf-8\")\n",
    "\n",
    "len(text), text[:300]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "48bf23cd-06ab-43b2-b2be-23dccf498cfb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(spacy.tokens.doc.Doc, 19720)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the spaCy Doc (this runs tokenization, POS, NER, etc.)\n",
    "doc = nlp(text)\n",
    "\n",
    "type(doc), len(doc)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "51f3a2c5-cb76-4c69-969a-f3f0bee7d230",
   "metadata": {},
   "outputs": [],
   "source": [
    "with nlp.select_pipes(enable=[\"ner\"]):\n",
    "    doc = nlp(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f2174284-d08c-445d-85f3-c921d8984fcd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the 20th century', 'DATE'),\n",
       " ('1900s\\n1901\\nJanuary 1', 'DATE'),\n",
       " ('Australian', 'NORP'),\n",
       " ('January 22', 'DATE'),\n",
       " ('King of England', 'ORG'),\n",
       " ('India', 'GPE'),\n",
       " (\"Queen Victoria 's\", 'PERSON'),\n",
       " ('March 2', 'DATE'),\n",
       " ('Cuban', 'NORP'),\n",
       " ('American', 'NORP'),\n",
       " ('June', 'DATE'),\n",
       " ('45', 'CARDINAL'),\n",
       " ('British', 'NORP'),\n",
       " ('South Africa', 'GPE'),\n",
       " ('September 6', 'DATE'),\n",
       " ('William McKinley', 'PERSON'),\n",
       " ('Theodore Roosevelt', 'PERSON'),\n",
       " ('McKinley', 'GPE'),\n",
       " ('September 14', 'DATE'),\n",
       " ('September 7', 'DATE'),\n",
       " ('Eight', 'CARDINAL'),\n",
       " ('the Boxer Rebellion', 'ORG'),\n",
       " ('China', 'GPE'),\n",
       " ('December 10', 'DATE'),\n",
       " ('December 12', 'DATE')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show a few entities with labels\n",
    "[(ent.text, ent.label_) for ent in list(doc.ents)[:25]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fab97de6-bfe5-4e22-977e-55a61a56f2e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1146,\n",
       " [('Australian', 'NORP'),\n",
       "  ('India', 'GPE'),\n",
       "  ('Cuban', 'NORP'),\n",
       "  ('American', 'NORP'),\n",
       "  ('British', 'NORP'),\n",
       "  ('South Africa', 'GPE'),\n",
       "  ('McKinley', 'GPE'),\n",
       "  ('China', 'GPE'),\n",
       "  ('Cuba', 'GPE'),\n",
       "  ('the United States', 'GPE'),\n",
       "  ('British', 'NORP'),\n",
       "  ('the United Kingdom', 'GPE'),\n",
       "  ('Venezuelan', 'NORP'),\n",
       "  ('Britain', 'GPE'),\n",
       "  ('Germany', 'GPE'),\n",
       "  ('Italy', 'GPE'),\n",
       "  ('Venezuela', 'GPE'),\n",
       "  ('Serbia', 'GPE'),\n",
       "  ('Russia', 'GPE'),\n",
       "  ('Bolsheviks', 'GPE'),\n",
       "  ('Mensheviks', 'NORP'),\n",
       "  ('Panama', 'GPE'),\n",
       "  ('the United States', 'GPE'),\n",
       "  ('Panama', 'GPE'),\n",
       "  ('The Ottoman Empire', 'GPE')])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "country_like = [(ent.text, ent.label_) for ent in doc.ents if ent.label_ in {\"GPE\",\"LOC\",\"NORP\"}]\n",
    "len(country_like), country_like[:25]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9862c7f7-a047-46d7-bf44-5110f12e9711",
   "metadata": {},
   "source": [
    "Focus on countries like entities. spaCy marks country/place names mostly as:\n",
    "- GPE (countries/cities)\n",
    "- LOC (locations)\n",
    "- NORP (nationalities/religious/political groups: “French”, “Soviets”)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a5802466-739f-409d-a60d-7ff5465b3a91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences: 1003\n",
      "Example sentence: This is a timeline of the 20th century .\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'sent_id': 1,\n",
       "  'sentence': '1900s\\n1901\\nJanuary 1 : The Australian colonies federate .\\n',\n",
       "  'entities': ['Australian']},\n",
       " {'sent_id': 2,\n",
       "  'sentence': \"January 22 : Edward VII became King of England and India after Queen Victoria 's death.\\n\",\n",
       "  'entities': ['India']},\n",
       " {'sent_id': 3,\n",
       "  'sentence': 'March 2 : The Platt Amendment provides for Cuban independence in exchange for the withdrawal of American troops.\\n',\n",
       "  'entities': ['Cuban', 'American']}]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Load spaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Add a sentencizer if not already in the pipeline\n",
    "if \"senter\" not in nlp.pipe_names and \"sentencizer\" not in nlp.pipe_names:\n",
    "    nlp.add_pipe(\"sentencizer\")\n",
    "\n",
    "# Re-run NER on your text\n",
    "with open(\"20th_century_events.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "doc = nlp(text)\n",
    "\n",
    "# Ensure sentences are now available\n",
    "sents = list(doc.sents)\n",
    "print(\"Number of sentences:\", len(sents))\n",
    "print(\"Example sentence:\", sents[0].text)\n",
    "\n",
    "# Extract GPE / LOC / NORP mentions\n",
    "sent_entities = []\n",
    "for i, s in enumerate(sents):\n",
    "    ents = {ent.text for ent in s.ents if ent.label_ in {\"GPE\", \"LOC\", \"NORP\"}}\n",
    "    if ents:\n",
    "        sent_entities.append({\n",
    "            \"sent_id\": i,\n",
    "            \"sentence\": s.text,\n",
    "            \"entities\": list(ents)\n",
    "        })\n",
    "\n",
    "sent_entities[:3]  # preview first 3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08bb33ed-c730-4554-8d51-f861ec9a952f",
   "metadata": {},
   "source": [
    "## Entities into a DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "893d6132-733b-476d-b4fe-58716d71aef0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows with at least one entity: 633\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "ner_df = pd.DataFrame(sent_entities)  # list of dicts: sent_id, sentence, entities(list)\n",
    "ner_df.head(10)\n",
    "print(\"Rows with at least one entity:\", len(ner_df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7aa26d8a-d991-41d8-96fc-9d51d3074916",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total entity mentions (sentence-level): 1144\n"
     ]
    }
   ],
   "source": [
    "ner_long = (\n",
    "    ner_df\n",
    "    .explode(\"entities\", ignore_index=True)\n",
    "    .rename(columns={\"entities\": \"entity\"})\n",
    ")\n",
    "ner_long.head(10)\n",
    "print(\"Total entity mentions (sentence-level):\", len(ner_long))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6e2296de-e3d5-489e-b54a-5274e32e9e52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique edges: 827\n"
     ]
    }
   ],
   "source": [
    "from itertools import combinations\n",
    "\n",
    "# helper: unique, case-normalized entities per sentence (avoid duplicates like \"UK\" and \"United Kingdom\" if you normalized earlier)\n",
    "def norm(x): \n",
    "    return x.strip()\n",
    "\n",
    "pairs = []\n",
    "for row in ner_df.itertuples(index=False):\n",
    "    ents = sorted({norm(e) for e in row.entities})\n",
    "    if len(ents) >= 2:\n",
    "        pairs.extend(list(combinations(ents, 2)))  # unordered pairs within the sentence\n",
    "\n",
    "edges_df = (\n",
    "    pd.DataFrame(pairs, columns=[\"src\", \"dst\"])\n",
    "    .value_counts()\n",
    "    .reset_index(name=\"weight\")\n",
    "    .sort_values(\"weight\", ascending=False)\n",
    ")\n",
    "edges_df.head(15)\n",
    "print(\"Unique edges:\", len(edges_df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "14662355-4389-4d9c-91b1-e44b34d14ca1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Country-only edges: 46\n"
     ]
    }
   ],
   "source": [
    "# Example: keep only entities that match your canonical set\n",
    "canonical = set([\n",
    "    \"united states\",\"united kingdom\",\"germany\",\"france\",\"italy\",\"spain\",\n",
    "    \"russia\",\"soviet union\",\"china\",\"japan\",\"india\",\"canada\",\"australia\",\n",
    "    \"poland\",\"austria\",\"hungary\",\"netherlands\",\"belgium\",\"switzerland\",\n",
    "    \"sweden\",\"norway\",\"denmark\",\"finland\",\"greece\",\"portugal\",\"ireland\",\n",
    "    \"czechoslovakia\",\"yugoslavia\",\"turkey\",\"egypt\",\"iran\",\"iraq\",\"israel\",\n",
    "    \"mexico\",\"brazil\",\"argentina\",\"chile\",\"south africa\",\"pakistan\",\n",
    "    \"south korea\",\"north korea\",\"vietnam\",\"indonesia\",\"philippines\",\"thailand\"\n",
    "])\n",
    "\n",
    "# Lowercase columns and filter\n",
    "edges_df_lc = edges_df.assign(\n",
    "    src=edges_df[\"src\"].str.lower(),\n",
    "    dst=edges_df[\"dst\"].str.lower()\n",
    ")\n",
    "edges_countries = edges_df_lc[\n",
    "    edges_df_lc[\"src\"].isin(canonical) & edges_df_lc[\"dst\"].isin(canonical)\n",
    "].copy()\n",
    "\n",
    "edges_countries.sort_values(\"weight\", ascending=False).head(15)\n",
    "print(\"Country-only edges:\", len(edges_countries))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b98f9acb-af3f-4171-9746-8508ec6dc7fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: ner_sentences.csv, ner_mentions_long.csv, entity_cooccurrence_edges.csv, country_cooccurrence_edges.csv\n"
     ]
    }
   ],
   "source": [
    "ner_df.to_csv(\"ner_sentences.csv\", index=False)\n",
    "ner_long.to_csv(\"ner_mentions_long.csv\", index=False)\n",
    "edges_df.to_csv(\"entity_cooccurrence_edges.csv\", index=False)\n",
    "edges_countries.to_csv(\"country_cooccurrence_edges.csv\", index=False)\n",
    "\n",
    "print(\"Saved: ner_sentences.csv, ner_mentions_long.csv, entity_cooccurrence_edges.csv, country_cooccurrence_edges.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "61fff82b-fb4b-43a1-abba-01e96c53e3e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import spacy\n",
    "from itertools import combinations\n",
    "import pandas as pd\n",
    "\n",
    "# Ensure sentencizer exists so doc.sents works\n",
    "if \"senter\" not in nlp.pipe_names and \"sentencizer\" not in nlp.pipe_names:\n",
    "    nlp.add_pipe(\"sentencizer\")\n",
    "\n",
    "# ---- canonical list (lowercase) ----\n",
    "countries = [\n",
    "    \"united states\",\"united kingdom\",\"germany\",\"france\",\"italy\",\"spain\",\n",
    "    \"russia\",\"soviet union\",\"china\",\"japan\",\"india\",\"canada\",\"australia\",\n",
    "    \"poland\",\"austria\",\"hungary\",\"netherlands\",\"belgium\",\"switzerland\",\n",
    "    \"sweden\",\"norway\",\"denmark\",\"finland\",\"greece\",\"portugal\",\"ireland\",\n",
    "    \"czechoslovakia\",\"yugoslavia\",\"turkey\",\"egypt\",\"iran\",\"iraq\",\"israel\",\n",
    "    \"mexico\",\"brazil\",\"argentina\",\"chile\",\"south africa\",\"pakistan\",\n",
    "    \"south korea\",\"north korea\",\"vietnam\",\"indonesia\",\"philippines\",\"thailand\"\n",
    "]\n",
    "countries_set = set(countries)\n",
    "\n",
    "# ---- aliases -> canonical (all lowercase) ----\n",
    "aliases = {\n",
    "    \"usa\":\"united states\",\"u.s.\":\"united states\",\"u.s.a.\":\"united states\",\"america\":\"united states\",\n",
    "    \"uk\":\"united kingdom\",\"u.k.\":\"united kingdom\",\"britain\":\"united kingdom\",\"great britain\":\"united kingdom\",\"england\":\"united kingdom\",\n",
    "    \"ussr\":\"soviet union\",\"u.s.s.r.\":\"soviet union\",\"union of soviet socialist republics\":\"soviet union\",\n",
    "    \"west germany\":\"germany\",\"east germany\":\"germany\",\"frg\":\"germany\",\"gdr\":\"germany\",\n",
    "    \"prc\":\"china\",\"people's republic of china\":\"china\",\"roc\":\"china\",\"republic of china\":\"china\",\n",
    "    \"ottoman empire\":\"turkey\",\"ottoman\":\"turkey\",\n",
    "    \"siam\":\"thailand\",\"holland\":\"netherlands\",\n",
    "}\n",
    "\n",
    "# Normalize an entity string to its canonical country (or None if not a country)\n",
    "def canon_country(s: str) -> str | None:\n",
    "    x = s.strip().lower()\n",
    "    # replace aliases first (longest aliases first to avoid partial overlaps)\n",
    "    for alias, canon in sorted(aliases.items(), key=lambda kv: len(kv[0]), reverse=True):\n",
    "        if re.fullmatch(rf\"{re.escape(alias)}\", x):\n",
    "            x = aliases[alias]\n",
    "            break\n",
    "    return x if x in countries_set else None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a1f0caef-3df5-454f-a8a7-4394533159cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(633,\n",
       " [{'sent_id': 1,\n",
       "   'sentence': '1900s\\n1901\\nJanuary 1 : The Australian colonies federate .\\n',\n",
       "   'raw_entities': ['Australian']},\n",
       "  {'sent_id': 2,\n",
       "   'sentence': \"January 22 : Edward VII became King of England and India after Queen Victoria 's death.\\n\",\n",
       "   'raw_entities': ['India']}])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sentence list\n",
    "sents = list(doc.sents)\n",
    "\n",
    "# For each sentence, keep only named entities of type GPE/LOC/NORP (place-like)\n",
    "sent_entities = []\n",
    "for i, s in enumerate(sents):\n",
    "    ents = [ent.text for ent in s.ents if ent.label_ in {\"GPE\",\"LOC\",\"NORP\"}]\n",
    "    if ents:\n",
    "        sent_entities.append({\"sent_id\": i, \"sentence\": s.text, \"raw_entities\": ents})\n",
    "\n",
    "len(sent_entities), sent_entities[:2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "62b0b91e-8632-4c06-b134-211d21175720",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(267,\n",
       " [{'sent_id': 2,\n",
       "   'sentence': \"January 22 : Edward VII became King of England and India after Queen Victoria 's death.\\n\",\n",
       "   'raw_entities': ['India'],\n",
       "   'countries': ['india']},\n",
       "  {'sent_id': 4,\n",
       "   'sentence': \"June : Emily Hobhouse reports on the poor conditions in 45 British internment camps for Boer women and children in South Africa .\\nSeptember 6 : The assassination of William McKinley ushered in office Vice President Theodore Roosevelt after McKinley's death on September 14.\\nSeptember 7 : The Eight-Nation Alliance defeats the Boxer Rebellion , and imposes heavy financial penalties on China .\\n\",\n",
       "   'raw_entities': ['British', 'South Africa', 'McKinley', 'China'],\n",
       "   'countries': ['china', 'south africa']}])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Map sentence entities -> canonical countries; drop non-countries\n",
    "for row in sent_entities:\n",
    "    canon_ents = {c for e in row[\"raw_entities\"] if (c := canon_country(e))}\n",
    "    row[\"countries\"] = sorted(canon_ents)\n",
    "\n",
    "# Keep sentences that actually contain >= 2 distinct countries (useful for relationships)\n",
    "sent_with_countries = [r for r in sent_entities if len(r[\"countries\"]) >= 1]\n",
    "len(sent_with_countries), sent_with_countries[:2]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6bf248f-39b2-4fd8-8912-53a267667ea3",
   "metadata": {},
   "source": [
    "## Create the relationships dataframe (country co-occurrences)\n",
    "- Within each sentence, we connect every unique pair of countries.\n",
    "- Then we aggregate across all sentences to get an edge weight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2c6daeb2-4fa3-4f39-acb1-eeaa28faa831",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(     source          target  weight\n",
       " 0   germany  united kingdom       4\n",
       " 1      iran   united states       3\n",
       " 2    france  united kingdom       3\n",
       " 3     china           japan       2\n",
       " 4   austria          sweden       2\n",
       " 5   finland          sweden       2\n",
       " 6   austria         hungary       2\n",
       " 7   denmark          norway       2\n",
       " 8   austria         finland       2\n",
       " 9    france         germany       2\n",
       " 10   poland          russia       2\n",
       " 11   france     netherlands       1\n",
       " 12  austria          russia       1\n",
       " 13  belgium         denmark       1\n",
       " 14  belgium          france       1,\n",
       " 57)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairs = []\n",
    "for r in sent_with_countries:\n",
    "    # unique per sentence\n",
    "    cs = sorted(set(r[\"countries\"]))\n",
    "    # choose 2 for undirected pairs; if only one country, skip\n",
    "    if len(cs) >= 2:\n",
    "        pairs += list(combinations(cs, 2))\n",
    "\n",
    "# Build edge table with weights\n",
    "edges_df = (\n",
    "    pd.DataFrame(pairs, columns=[\"source\",\"target\"])\n",
    "      .value_counts()\n",
    "      .reset_index(name=\"weight\")\n",
    "      .sort_values(\"weight\", ascending=False)\n",
    "      .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "edges_df.head(15), len(edges_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9bc90c92-69b3-4b32-b6a5-36b71d057181",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>country</th>\n",
       "      <th>degree_weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>france</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>united kingdom</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>germany</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>austria</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>china</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>united states</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>denmark</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>poland</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>sweden</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>norway</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           country  degree_weight\n",
       "9           france             12\n",
       "31  united kingdom             12\n",
       "10         germany             12\n",
       "0          austria             10\n",
       "4            china              9\n",
       "32   united states              8\n",
       "6          denmark              7\n",
       "21          poland              6\n",
       "28          sweden              6\n",
       "19          norway              6"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Node degrees (weighted)\n",
    "node_weights = (\n",
    "    edges_df\n",
    "      .assign(pair_weight=lambda d: d[\"weight\"])\n",
    "      .melt(id_vars=\"weight\", value_vars=[\"source\",\"target\"], value_name=\"country\")\n",
    "      .groupby(\"country\", as_index=False)[\"weight\"].sum()\n",
    "      .rename(columns={\"weight\":\"degree_weight\"})\n",
    "      .sort_values(\"degree_weight\", ascending=False)\n",
    ")\n",
    "node_weights.head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9755da8-696d-40b1-a969-285978122abc",
   "metadata": {},
   "source": [
    "## Save & export your dataframe(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fe83abdf-bb31-4e9e-ae15-dd75a9bf291c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved:\n",
      " • sentences_with_countries.csv\n",
      " • country_relationships_edges.csv\n",
      " • country_relationships_nodes.csv\n"
     ]
    }
   ],
   "source": [
    "edges_path = \"country_relationships_edges.csv\"\n",
    "nodes_path = \"country_relationships_nodes.csv\"\n",
    "sent_path  = \"sentences_with_countries.csv\"\n",
    "\n",
    "pd.DataFrame(sent_with_countries).to_csv(sent_path, index=False)\n",
    "edges_df.to_csv(edges_path, index=False)\n",
    "node_weights.to_csv(nodes_path, index=False)\n",
    "\n",
    "print(\"Saved:\")\n",
    "print(\" •\", sent_path)\n",
    "print(\" •\", edges_path)\n",
    "print(\" •\", nodes_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4951618-8a86-422f-9714-0b1df796b0f0",
   "metadata": {},
   "source": [
    "- sentences_with_countries.csv – each sentence + the countries found there\n",
    "- country_relationships_edges.csv – (source, target, weight) for co-occurrences\n",
    "- country_relationships_nodes.csv – node list with a simple degree_weight (sum of connected edge weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c319628-827c-423e-8a23-512afe85001b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (20th_century)",
   "language": "python",
   "name": "20th_century"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
